{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final - develop_word_embeddings_python_gensim.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGMX97NcEbWI"
      },
      "source": [
        "# Word Embeddings with Gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldkfQlDaHr8D"
      },
      "source": [
        "## Word2vec by Google and GloVe by Stanford"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihnc0sVvu9H1"
      },
      "source": [
        "Word embeddings are an **improvement over simpler bag-of-words model** word encoding schemes like word counts and frequencies that result in large and sparse vectors (mostly 0 values) that describe documents but not the meaning of the words. It provides a **dense vector representation of words that capture something about their meaning**.\n",
        "\n",
        "It is **defining a word by the company that it keeps** that allows the word embedding to learn something about the meaning of words. The vector space representation of the words provides a projection where words with similar meanings are locally clustered within the space.\n",
        "\n",
        "We are going to look at how to use two different word embedding methods called **word2vec by researchers at Google and GloVe by researchers at Stanford**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33sO1pYiIEAy"
      },
      "source": [
        "## Gensim library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT7i5Snfu9IE"
      },
      "source": [
        "Gensim is a mature, focused, and efficient suite of NLP tools for topic modeling\n",
        "1. It supports an implementation of the Word2Vec word embedding for **learning new word vectors** from text\n",
        "2. It also provides tools for **loading pre-trained word embeddings** in a few formats and for making use and querying a loaded embedding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuZCNtyRUTZ5"
      },
      "source": [
        "Research Paper: https://arxiv.org/pdf/1301.3781.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je5lFkeyvBRl",
        "outputId": "12e6e388-c20f-4908-9f96-27d1f2995b94"
      },
      "source": [
        "!pip install nltk\n",
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.1.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJUQOg-fu9H5"
      },
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJxIGSmZUu5K"
      },
      "source": [
        "#Data Requirement :\n",
        "# Genism word2vec requires that a format of ‘list of lists’ for training \n",
        "# where every document is contained in a list and every list contains lists \n",
        "# of tokens of that document. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qCeQ8xdu9IG"
      },
      "source": [
        "# define training data\n",
        "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
        "            ['this', 'is', 'the', 'second', 'sentence'],\n",
        "            ['yet', 'another', 'sentence'],\n",
        "            ['one', 'more', 'sentence'],\n",
        "            ['and', 'the', 'final', 'sentence']]\n",
        "\n",
        "# train model\n",
        "model = Word2Vec(sentences, min_count=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBN6N-VIu9IR"
      },
      "source": [
        "There are many parameters on this constructor for **Word2Vec()**; a few noteworthy arguments you may wish to configure are:\n",
        "* **size**: (default 100) The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token (word).\n",
        "* **window**: (default 5) The maximum distance between a target word and words around the target word.\n",
        "* **min_count**: (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n",
        "* **sg**: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX0j2-n49L-_"
      },
      "source": [
        "https://radimrehurek.com/gensim/models/word2vec.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0tP8fNGu9IS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61055fa3-93ce-46e2-c135-f8138084f114"
      },
      "source": [
        "# summarize the loaded model\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=14, size=100, alpha=0.025)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1ZENYmru9Ic"
      },
      "source": [
        "After the model is trained, it is accessible via the “wv” attribute. This is the actual word vector model in which queries can be made"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZN11YyPu9Ie",
        "outputId": "b225c1a6-b778-4065-c03c-7a1f8751e0c3"
      },
      "source": [
        "# summarize vocabulary\n",
        "words = list(model.wv.vocab)\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec', 'second', 'yet', 'another', 'one', 'more', 'and', 'final']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqhQf_gvu9Io",
        "outputId": "13919b25-fb14-4b99-e9d1-6c49ff1b9466"
      },
      "source": [
        "# access vector for one word\n",
        "print(model.wv['this'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-4.3893862e-03  3.7961050e-03  4.6180272e-03 -1.2384416e-03\n",
            " -3.7356431e-03  4.2216345e-03 -2.9612479e-03 -1.8302110e-03\n",
            "  3.6823978e-03  4.0696221e-03  1.3373365e-03 -8.3006092e-04\n",
            " -9.2130020e-04  2.2681216e-03  2.7022394e-03  3.1971852e-03\n",
            " -2.6186912e-03 -3.1495015e-03  3.9227861e-03 -1.9460836e-03\n",
            " -2.3027512e-03  4.9813343e-03 -2.3493753e-03 -1.3224503e-03\n",
            "  4.8802290e-03  6.9500098e-04 -4.8406135e-05 -9.4956590e-04\n",
            " -6.0881230e-05 -1.5883758e-03 -2.6180863e-03  2.0279612e-03\n",
            "  4.9604066e-03 -4.0868670e-03 -4.7215814e-04 -1.6543757e-03\n",
            " -4.7288765e-03 -7.8805478e-04  5.1260885e-04 -2.8562951e-03\n",
            "  1.4741262e-03  4.4005169e-03  4.9622855e-03  3.6938949e-03\n",
            " -1.0272657e-03 -4.8746476e-03  2.9128657e-03  2.4387762e-03\n",
            " -4.8734355e-03 -1.6346725e-03 -2.7242315e-03  4.3639466e-03\n",
            " -2.3159257e-03  9.5855270e-04  2.4707962e-03  6.1542093e-04\n",
            " -1.7018864e-03 -8.5802545e-04 -1.3015893e-03  2.5290588e-03\n",
            "  2.2975812e-03  2.8553384e-03 -2.8669008e-03  1.7357068e-03\n",
            " -1.9255104e-03 -2.2543231e-03 -1.8276327e-03  4.1655810e-03\n",
            "  3.4259695e-03 -4.3732012e-03  4.9295900e-03 -2.2197952e-03\n",
            "  4.7119143e-03 -2.3735950e-03  2.7347929e-03  2.4355412e-03\n",
            "  3.6636363e-03  2.3131368e-04  2.1655415e-03 -6.9362111e-04\n",
            " -4.4146050e-03 -1.4066333e-03  2.3491809e-03  4.6110810e-03\n",
            "  3.2198623e-05 -3.9481092e-03  1.9732364e-03 -3.6427458e-03\n",
            "  3.1167199e-03  4.0354449e-03 -1.6339516e-03  3.7515704e-03\n",
            "  3.8107643e-03 -3.2646477e-03  3.9163381e-03  3.6673425e-04\n",
            " -2.3999180e-04 -3.4481038e-03 -4.3540141e-03  4.0533249e-03]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3aR9-k-SQSU",
        "outputId": "c9b5e5b0-7f50-4140-df9a-06f4ce516d3e"
      },
      "source": [
        "# access vector for one word\n",
        "print(len(model.wv['this']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l3oxL2DVcyg",
        "outputId": "20853d13-45a1-4c1a-8837-3e216b5b3624"
      },
      "source": [
        "#Cosine Similarity between words\n",
        "model.wv.similarity('more',\"second\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.1077418"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "870ocI1lVykY",
        "outputId": "3f158643-ebba-4936-8f21-2c4a47b40883"
      },
      "source": [
        "#Find Cosine Similarity between two sentences\n",
        "model.wv.n_similarity(sentences[0],sentences[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.66410255"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVt0RfFsXPsS",
        "outputId": "9517be84-0180-4a7f-99fc-e233c841a9d9"
      },
      "source": [
        "model.wv.most_similar(\"first\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('final', 0.15396729111671448),\n",
              " ('the', 0.13147643208503723),\n",
              " ('yet', 0.09594659507274628),\n",
              " ('second', 0.0774758830666542),\n",
              " ('sentence', 0.06391753256320953),\n",
              " ('and', 0.06266756355762482),\n",
              " ('for', 0.058620207011699677),\n",
              " ('one', 0.05060993880033493),\n",
              " ('more', 0.045300088822841644),\n",
              " ('is', -0.013383306562900543)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMG46lfOu9Ix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bce352c9-f16f-473b-f6bf-2ffb0b5498e3"
      },
      "source": [
        "# save model\n",
        "model.save('model.bin')\n",
        "# load model\n",
        "new_model = Word2Vec.load('model.bin')\n",
        "print(new_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=14, size=100, alpha=0.025)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X4MqHCTu9Js"
      },
      "source": [
        "## Load Google’s Word2Vec Embedding\n",
        "Training your own word vectors may be the best approach for a given NLP problem. But it can take a long time, a fast computer with a lot of RAM and disk space, and perhaps some expertise in finessing the input data and training algorithm.\n",
        "\n",
        "An alternative is to simply use an existing pre-trained word embedding. Along with the paper and code for word2vec, Google also published a pre-trained word2vec model on the <a href='https://code.google.com/archive/p/word2vec/'>Word2Vec Google Code Project</a>\n",
        "\n",
        "A pre-trained model is nothing more than a file containing tokens and their associated word vectors. The pre-trained Google word2vec model was trained on Google news data (about 100 billion words); it contains 3 million words and phrases and was fit using 300-dimensional word vectors. It is a 1.53 Gigabytes file. You can download it from here: <a href='https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing'>GoogleNews-vectors-negative300.bin.gz</a>.  Unzipped, the binary file (GoogleNews-vectors-negative300.bin) is 3.4 Gigabytes.\n",
        "\n",
        "We can load directly from genism package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q788wpTS4Ab",
        "outputId": "b3b96633-3527-4533-84e4-4bc0d08fde43"
      },
      "source": [
        "import gensim.downloader\n",
        "print(list(gensim.downloader.info()['models'].keys()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58EHvR-6S6wc",
        "outputId": "a22d44ef-347f-4667-df4f-2d889826004e"
      },
      "source": [
        "# Download the \"word2vec-google-news-300\" embeddings\n",
        "glove_vectors = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr71wEKmu9J1"
      },
      "source": [
        "An interesting thing that you can do is do a little linear algebra arithmetic with words. For example, a popular example described in lectures and introduction papers is: queen = (king - man) + woman\n",
        "\n",
        "That is the word queen is the closest word given the subtraction of the notion of man from king and adding the word woman. The “man-ness” in king is replaced with “woman-ness” to give us queen. A very cool concept.\n",
        "\n",
        "Gensim provides an interface for performing these types of operations in the **most_similar()** function on the trained or loaded model. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "799jPLOrTOAp",
        "outputId": "61e5c473-c6ab-4790-a140-45c29ab1197b"
      },
      "source": [
        "glove_vectors.most_similar('king')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('kings', 0.7138046026229858),\n",
              " ('queen', 0.6510956883430481),\n",
              " ('monarch', 0.6413194537162781),\n",
              " ('crown_prince', 0.6204220056533813),\n",
              " ('prince', 0.6159993410110474),\n",
              " ('sultan', 0.5864822864532471),\n",
              " ('ruler', 0.5797567367553711),\n",
              " ('princes', 0.5646552443504333),\n",
              " ('Prince_Paras', 0.543294370174408),\n",
              " ('throne', 0.5422104597091675)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvrqJi6RaEXh",
        "outputId": "64cfb07a-0af7-4dfb-ea94-04f79abf329f"
      },
      "source": [
        "glove_vectors['queen']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.00524902, -0.14355469, -0.06933594,  0.12353516,  0.13183594,\n",
              "       -0.08886719, -0.07128906, -0.21679688, -0.19726562,  0.05566406,\n",
              "       -0.07568359, -0.38085938,  0.10400391, -0.00081635,  0.1328125 ,\n",
              "        0.11279297,  0.07275391, -0.046875  ,  0.06591797,  0.09423828,\n",
              "        0.19042969,  0.13671875, -0.23632812, -0.11865234,  0.06542969,\n",
              "       -0.05322266, -0.30859375,  0.09179688,  0.18847656, -0.16699219,\n",
              "       -0.15625   , -0.13085938, -0.08251953,  0.21289062, -0.35546875,\n",
              "       -0.13183594,  0.09619141,  0.26367188, -0.09472656,  0.18359375,\n",
              "        0.10693359, -0.41601562,  0.26953125, -0.02770996,  0.17578125,\n",
              "       -0.11279297, -0.00411987,  0.14550781,  0.15625   ,  0.26757812,\n",
              "       -0.01794434,  0.09863281,  0.05297852, -0.03125   , -0.16308594,\n",
              "       -0.05810547, -0.34375   , -0.17285156,  0.11425781, -0.09033203,\n",
              "        0.13476562,  0.27929688, -0.04980469,  0.12988281,  0.17578125,\n",
              "       -0.22167969, -0.01190186,  0.140625  , -0.18164062,  0.11865234,\n",
              "        0.16113281,  0.21484375, -0.21191406,  0.12695312, -0.10009766,\n",
              "        0.13671875,  0.12695312,  0.01531982,  0.10449219, -0.02783203,\n",
              "       -0.06030273,  0.0222168 ,  0.18164062, -0.06738281,  0.04907227,\n",
              "        0.15429688, -0.25      ,  0.13964844,  0.29492188,  0.10644531,\n",
              "        0.3359375 , -0.22265625, -0.125     , -0.05297852,  0.19238281,\n",
              "        0.06835938,  0.06982422, -0.05200195,  0.14453125,  0.00448608,\n",
              "       -0.01013184, -0.1484375 ,  0.21777344, -0.1953125 , -0.390625  ,\n",
              "        0.07763672, -0.57421875, -0.07910156, -0.04052734, -0.1875    ,\n",
              "        0.25390625,  0.15722656,  0.125     ,  0.140625  ,  0.20117188,\n",
              "       -0.05859375,  0.16894531, -0.28125   ,  0.171875  ,  0.19140625,\n",
              "        0.12109375, -0.15039062, -0.00695801, -0.23730469,  0.13964844,\n",
              "       -0.00836182, -0.04711914,  0.14648438, -0.05688477,  0.10205078,\n",
              "        0.08447266,  0.21191406, -0.01831055,  0.50390625, -0.04858398,\n",
              "        0.22167969, -0.25585938,  0.03417969,  0.15820312, -0.03369141,\n",
              "        0.06738281, -0.25195312,  0.04614258, -0.07275391,  0.07958984,\n",
              "        0.04223633, -0.00128937,  0.20214844, -0.13085938, -0.06030273,\n",
              "        0.0378418 ,  0.13574219,  0.11181641, -0.24609375, -0.23925781,\n",
              "       -0.23632812, -0.04321289, -0.02905273,  0.23535156, -0.00390625,\n",
              "       -0.05029297,  0.18457031,  0.50390625, -0.00668335, -0.03466797,\n",
              "       -0.07568359,  0.06152344, -0.31445312, -0.03759766,  0.23632812,\n",
              "       -0.12792969,  0.15429688,  0.296875  ,  0.02709961, -0.17089844,\n",
              "       -0.22460938,  0.00241089,  0.10595703, -0.03320312,  0.0145874 ,\n",
              "       -0.21582031,  0.24707031, -0.07421875, -0.10205078,  0.16894531,\n",
              "       -0.05029297,  0.20800781, -0.03857422, -0.22265625,  0.27539062,\n",
              "       -0.05957031, -0.01757812,  0.01794434,  0.08886719,  0.12890625,\n",
              "        0.18261719,  0.14453125,  0.10400391, -0.1328125 , -0.32617188,\n",
              "        0.00386047, -0.11376953, -0.05053711, -0.13085938,  0.02209473,\n",
              "       -0.14648438,  0.10742188,  0.23046875,  0.15234375,  0.22753906,\n",
              "        0.04833984,  0.06787109, -0.06787109, -0.2578125 ,  0.11230469,\n",
              "        0.00363159, -0.12011719, -0.21289062,  0.11230469,  0.12158203,\n",
              "        0.06835938,  0.04907227,  0.2734375 , -0.00302124, -0.00378418,\n",
              "        0.00193787,  0.1875    , -0.29101562,  0.09033203,  0.26367188,\n",
              "       -0.25585938, -0.28710938, -0.40820312,  0.10546875,  0.39648438,\n",
              "       -0.07275391, -0.04321289, -0.06347656, -0.00060272, -0.11523438,\n",
              "        0.31445312, -0.22265625,  0.13574219, -0.01965332,  0.15332031,\n",
              "        0.00360107, -0.12011719,  0.06494141,  0.16210938, -0.16699219,\n",
              "        0.03271484, -0.00350952,  0.18847656,  0.19335938,  0.1328125 ,\n",
              "        0.06787109, -0.34179688, -0.08349609, -0.29492188, -0.02099609,\n",
              "        0.08886719,  0.32421875, -0.36914062, -0.0859375 , -0.04956055,\n",
              "        0.13183594,  0.04418945,  0.359375  ,  0.21484375,  0.265625  ,\n",
              "       -0.2734375 ,  0.23535156,  0.11425781,  0.08789062,  0.1875    ,\n",
              "       -0.33203125,  0.15136719, -0.03613281, -0.11914062,  0.27734375,\n",
              "        0.10839844, -0.07275391,  0.23242188,  0.00219727,  0.23828125,\n",
              "       -0.24902344, -0.12353516, -0.15917969, -0.00601196,  0.14550781,\n",
              "       -0.00460815, -0.22558594, -0.37890625, -0.37695312, -0.08251953,\n",
              "       -0.04125977,  0.16796875, -0.046875  ,  0.16308594,  0.15429688],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mii1wDL9-_50",
        "outputId": "081fc356-cda7-4138-9bf7-30c7b71d9d2f"
      },
      "source": [
        "len(glove_vectors[\"queen\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f660U1_bGGl4"
      },
      "source": [
        "## Load Stanford’s GloVe Embedding\n",
        "Stanford researchers also have their own word embedding algorithm like word2vec called Global Vectors for Word Representation, or GloVe for short.Generally, NLP practitioners seem to prefer GloVe over Word2Vec at the moment based on results.\n",
        "\n",
        "Like word2vec, the GloVe researchers also provide pre-trained word vectors, in this case, a great selection to choose from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nxyh3K7sYrjg",
        "outputId": "3921cf9e-83e7-4fd0-9379-fd2ed0f799fc"
      },
      "source": [
        "# Download the 'glove-wiki-gigaword-50' embeddings\n",
        "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-50')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzRXvZtnZBIG",
        "outputId": "e284d34a-8337-4231-eea4-310ba9d529bd"
      },
      "source": [
        "glove_vectors['queen']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.37854  ,  1.8233   , -1.2648   , -0.1043   ,  0.35829  ,\n",
              "        0.60029  , -0.17538  ,  0.83767  , -0.056798 , -0.75795  ,\n",
              "        0.22681  ,  0.98587  ,  0.60587  , -0.31419  ,  0.28877  ,\n",
              "        0.56013  , -0.77456  ,  0.071421 , -0.5741   ,  0.21342  ,\n",
              "        0.57674  ,  0.3868   , -0.12574  ,  0.28012  ,  0.28135  ,\n",
              "       -1.8053   , -1.0421   , -0.19255  , -0.55375  , -0.054526 ,\n",
              "        1.5574   ,  0.39296  , -0.2475   ,  0.34251  ,  0.45365  ,\n",
              "        0.16237  ,  0.52464  , -0.070272 , -0.83744  , -1.0326   ,\n",
              "        0.45946  ,  0.25302  , -0.17837  , -0.73398  , -0.20025  ,\n",
              "        0.2347   , -0.56095  , -2.2839   ,  0.0092753, -0.60284  ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wQznlZ8_LKI",
        "outputId": "71cae3d6-d831-45af-8af2-c4213abe2d3a"
      },
      "source": [
        "len(glove_vectors['queen'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6aTqcaQZHSO",
        "outputId": "55dd1347-8c43-483c-f66f-ae2da179cde4"
      },
      "source": [
        "glove_vectors.most_similar(\"queen\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('princess', 0.851516604423523),\n",
              " ('lady', 0.805060863494873),\n",
              " ('elizabeth', 0.787304162979126),\n",
              " ('king', 0.7839042544364929),\n",
              " ('prince', 0.7821860909461975),\n",
              " ('coronation', 0.7692778706550598),\n",
              " ('consort', 0.7626097202301025),\n",
              " ('royal', 0.7442864775657654),\n",
              " ('crown', 0.7382649779319763),\n",
              " ('victoria', 0.7285771369934082)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHQhGMsRZRba",
        "outputId": "f98830a5-264d-4687-838b-8ed6654d6463"
      },
      "source": [
        "glove_vectors.most_similar(\"man\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('woman', 0.8860337734222412),\n",
              " ('boy', 0.8564431071281433),\n",
              " ('another', 0.8452839851379395),\n",
              " ('old', 0.8372182846069336),\n",
              " ('one', 0.8276063203811646),\n",
              " ('who', 0.8244696259498596),\n",
              " ('him', 0.8194693922996521),\n",
              " ('turned', 0.8154467940330505),\n",
              " ('whose', 0.8119741678237915),\n",
              " ('himself', 0.807725727558136)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gsBRpgyk6IM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "text-preprocessing-lesson-1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Iro4MZHs-HWs",
        "tnd15Vp1-HW1",
        "ikz79KdZ-HW6",
        "EoAhFjXM-HW8",
        "gFvsI2Bh-HXC",
        "romzrSQ9-HXO"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6gAqnm0-HVl"
      },
      "source": [
        "**What is NLP?**\n",
        "\n",
        "What can we do with NLP?\n",
        "\n",
        "* `information extraction`\n",
        "* `Text classification`\n",
        "   - Bag of words (tf-idf)\n",
        "   - deep learning (RNN/LSTM/Transformer)\n",
        "* `Language modeling/ Natural Text Generation`\n",
        "* `Text Similairty`\n",
        "> * `Topic modeling`\n",
        "* Translation\n",
        "* Chat bot\n",
        "* Question answering\n",
        "* Text-to-speech and Speech-to-text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB3hbXRs-HVn"
      },
      "source": [
        "The general workflow for any Natural Language Processing Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aqgp3j4q-HVp"
      },
      "source": [
        "![nlp_pract.PNG](attachment:nlp_pract.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AGK6d0n-HVq"
      },
      "source": [
        "* NLP\n",
        "* Intro to Kaggle Kernels / Jupyter Notebook\n",
        "* SpaCy- Text Tokenization, POS Tagging, Parsing, NER\n",
        "* Python Fundamentals: Collections, list comprehensions, sorted, apply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXNvW_pl-HVs"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btedtcp2-HVs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtZVYa7K-HVw",
        "outputId": "0021c4b2-5c59-44d2-94df-d24718fec0b0"
      },
      "source": [
        "2+2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70sduWWj-HV1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTdgmnqJ-HV2"
      },
      "source": [
        "                                                                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHZzyD_u-HV3"
      },
      "source": [
        "## SpaCy\n",
        "\n",
        "\"SpaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
        "\n",
        "If you're working with a lot of text, you'll eventually want to know more about it. For example, what's it about? What do the words mean in context? Who is doing what to whom? What companies and products are mentioned? Which texts are similar to each other?\n",
        "\n",
        "SpaCy is designed specifically for production use and helps you build applications that process and \"understand\" large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning.\n",
        "\n",
        "SpaCy is not research software. It's built on the latest research, but it's designed to get things done. This leads to fairly different design decisions than NLTK or CoreNLP, which were created as platforms for teaching and research. The main difference is that SpaCy is integrated and opinionated. SpaCy tries to avoid asking the user to choose between multiple algorithms that deliver equivalent functionality. Keeping the menu small lets SpaCy deliver generally better performance and developer experience.\"\n",
        "\n",
        "### SpaCy Features \n",
        "\n",
        "NAME |\tDESCRIPTION |\n",
        ":----- |:------|\n",
        "Tokenization|Segmenting text into words, punctuations marks etc.|\n",
        "Part-of-speech (POS) Tagging|Assigning word types to tokens, like verb or noun.|\n",
        "Dependency Parsing|\tAssigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.|\n",
        "Lemmatization|\tAssigning the base forms of words. For example, the lemma of \"was\" is \"be\", and the lemma of \"rats\" is \"rat\".|\n",
        "Sentence Boundary Detection (SBD)|\tFinding and segmenting individual sentences.|\n",
        "Named Entity Recognition (NER)|\tLabelling named \"real-world\" objects, like persons, companies or locations.|\n",
        "Similarity|\tComparing words, text spans and documents and how similar they are to each other.|\n",
        "Text Classification|\tAssigning categories or labels to a whole document, or parts of a document.|\n",
        "Rule-based Matching|\tFinding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.|\n",
        "Training|\tUpdating and improving a statistical model's predictions.|\n",
        "Serialization|\tSaving objects to files or byte strings.|\n",
        "\n",
        "SOURCE: https://spacy.io/usage/spacy-101****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHD-BP73-HV6"
      },
      "source": [
        "'He killed the man with **fire**'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_eQgWHM-HV7",
        "outputId": "528ddf5f-8e9d-482b-fe6c-c41012751fed"
      },
      "source": [
        "print('He killed the man with fire') "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "He killed the man with fire\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r2QKtbL-HV8"
      },
      "source": [
        "# importing spacy \n",
        "import spacy\n",
        "from spacy import displacy\n",
        "nlp = spacy.load('en')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSnUbwuw-HV-",
        "outputId": "e0560550-a2bb-4ef2-87ae-08b61de0645a"
      },
      "source": [
        "# review text\n",
        "\n",
        "text=\"This is one of the greatest films ever made. Brilliant acting by George C. Scott and Diane Riggs. This movie is both disturbing and extremely deep. Don't be fooled into believing this is just a comedy. It is a brilliant satire about the medical profession. It is not a pretty picture. Healthy patients are killed by incompetent surgeons, who spend all their time making money outside the hospital. And yet, you really believe that this is a hospital. The producers were very careful to include real medical terminology and real medical cases. This movie really reveals how difficult in is to run a hospital, and how badly things already were in 1971. I loved this movie.\"\n",
        "print(text)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is one of the greatest films ever made. Brilliant acting by George C. Scott and Diane Riggs. This movie is both disturbing and extremely deep. Don't be fooled into believing this is just a comedy. It is a brilliant satire about the medical profession. It is not a pretty picture. Healthy patients are killed by incompetent surgeons, who spend all their time making money outside the hospital. And yet, you really believe that this is a hospital. The producers were very careful to include real medical terminology and real medical cases. This movie really reveals how difficult in is to run a hospital, and how badly things already were in 1971. I loved this movie.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLidA1rW-HWA",
        "outputId": "511f23c8-24d4-40cc-be78-a66c2ca80075"
      },
      "source": [
        "# instantiate the document text\n",
        "doc = nlp(text)  #disable=['parser','tagger','ner'])\n",
        "# which the SpaCy document methods and attributes\n",
        "print(dir(doc))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '_bulk_merge', '_py_tokens', '_realloc', '_vector', '_vector_norm', 'cats', 'char_span', 'count_by', 'doc', 'ents', 'extend_tensor', 'from_array', 'from_bytes', 'from_disk', 'get_extension', 'get_lca_matrix', 'has_extension', 'has_vector', 'is_nered', 'is_parsed', 'is_sentenced', 'is_tagged', 'lang', 'lang_', 'mem', 'merge', 'noun_chunks', 'noun_chunks_iterator', 'print_tree', 'remove_extension', 'retokenize', 'sentiment', 'sents', 'set_extension', 'similarity', 'tensor', 'text', 'text_with_ws', 'to_array', 'to_bytes', 'to_disk', 'to_json', 'to_utf8_array', 'user_data', 'user_hooks', 'user_span_hooks', 'user_token_hooks', 'vector', 'vector_norm', 'vocab']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrHxwB-A-HWB"
      },
      "source": [
        "### NLP Pipeline\n",
        "\n",
        "When you read the text into spaCy, e.g. doc = nlp(text), you are applying a pipeline of nlp processes to the text.\n",
        "by default spaCy applies a tagger, parser, and ner, but you can choose to add, replace, or remove these steps.\n",
        "Note: Removing unnecessary steps for a given nlp can lead to substantial descreses in processing time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xp-2LvS-HWC",
        "outputId": "9b1aeb6a-cdd5-4726-911c-7172e2aaf471"
      },
      "source": [
        "from IPython.core.display import display, HTML\n",
        "# SpaCy pipeline\n",
        "spacy_url = 'https://spacy.io/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg'\n",
        "iframe = '<iframe src={} width=1000 height=200></iframe>'.format(spacy_url)\n",
        "HTML(iframe)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.6/site-packages/IPython/core/display.py:694: UserWarning: Consider using IPython.display.IFrame instead\n",
            "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe src=https://spacy.io/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg width=1000 height=200></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4CMlF3t-HWD"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "SpaCy first tokenizes the text, i.e. segments it into words, punctuation and so on. This is done by applying rules specific to each language. For example, punctuation at the end of a sentence should be split off – whereas \"U.K.\" should remain one token. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwF3-kpN-HWE",
        "outputId": "7453e612-0209-4528-f618-ca8573a76fd9"
      },
      "source": [
        "# SpaCy pipeline\n",
        "spacy_url = 'https://spacy.io/tokenization-57e618bd79d933c4ccd308b5739062d6.svg'\n",
        "iframe = '<iframe src={} width=1500 height=200></iframe>'.format(spacy_url)\n",
        "HTML(iframe)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe src=https://spacy.io/tokenization-57e618bd79d933c4ccd308b5739062d6.svg width=1500 height=200></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpvbZuZSE_sz",
        "outputId": "1950c4de-64ff-47b3-e1d0-c5bcdb772191"
      },
      "source": [
        "tokens = nlp(\" let's go to N.Y.!\")\n",
        "tokens"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " let's go to N.Y.!"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q21Vslv0FEkk",
        "outputId": "268a7ea5-d2ce-4f67-b2da-130c63188d96"
      },
      "source": [
        "for token in tokens:\n",
        "  print(token.text)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \n",
            "let\n",
            "'s\n",
            "go\n",
            "to\n",
            "N.Y.\n",
            "!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO55lfZW-HWG",
        "outputId": "029fd06a-f292-45cc-c6ad-e3016353224a"
      },
      "source": [
        "# \n",
        "[ token.text for token in nlp(\" let's go to N.Y.!\")],\" let's go to N.Y.!\".split()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([' ', 'let', \"'s\", 'go', 'to', 'N.Y.', '!'], [\"let's\", 'go', 'to', 'N.Y.!'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orh-khf8-HWH",
        "outputId": "c2f2d21e-0220-4593-beee-96e7d0c8d955"
      },
      "source": [
        "tok_doc=nlp(\"Some\\nspaces  and\\ttab characters\") # Let's go to N.Y.!'\n",
        "# tok_doc=nlp(\"Let's go to N.Y.!\")\n",
        "tokens_text = [t.text for t in tok_doc]\n",
        "tokens_text"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Some', '\\n', 'spaces', ' ', 'and', '\\t', 'tab', 'characters']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyfXHKWN-HWJ"
      },
      "source": [
        "##### STOPWORDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHrcy_X5-HWJ",
        "outputId": "c8dddfed-8444-456d-a32f-2a9694aadf98"
      },
      "source": [
        "# import a list of stop words from SpaCy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "print('Example stop words: {}'.format(list(STOP_WORDS)[0:10]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example stop words: ['being', 'as', 'was', 'hereby', 'me', 'ever', 'unless', 'n‘t', 'except', 'sometime']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcW8qWUj-HWK"
      },
      "source": [
        "#### Stemming and lemmatization\n",
        "\n",
        "from [Information Retrieval](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) textbook:\n",
        "\n",
        "Are the below words the same?\n",
        "\n",
        "`organize, organizes, and organizing`\n",
        "\n",
        "`democracy, democratic, and democratization`\n",
        "\n",
        "Stemming and Lemmatization both generate the root form of the words.\n",
        "\n",
        "Lemmatization uses the rules about a language. The resulting tokens are all actual words\n",
        "\n",
        "`\"Stemming is the poor-man’s lemmatization.\" (Noah Smith, 2011) Stemming is a crude heuristic that chops the ends off of words. \n",
        "The resulting tokens may not be actual words. Stemming is faster.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHE-EXnj-HWL"
      },
      "source": [
        "# word_list = ['feet', 'foot', 'foots', 'footing']\n",
        "word_list=['organize', 'organizes', 'organizing']"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUfrpV-E-HWM"
      },
      "source": [
        "from nltk import stem\n",
        "wnl = stem.WordNetLemmatizer()\n",
        "porter = stem.porter.PorterStemmer()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZW3FTqUG-HWN",
        "outputId": "e1ac49e8-bff8-48bc-e760-04faaa6cc2ab"
      },
      "source": [
        "[porter.stem(word) for word in word_list]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['organ', 'organ', 'organ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 994
        },
        "id": "4CxHE17g-HWO",
        "outputId": "753c2872-e727-4907-eb48-636b21e919ba"
      },
      "source": [
        "# Lemmatization using nltk\n",
        "[wnl.lemmatize(word) for word in word_list]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-ba21d9734eb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Lemmatization using nltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mwnl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-ba21d9734eb5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Lemmatization using nltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mwnl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzhTRDzb-HWP"
      },
      "source": [
        "**Spacy Lemmatization example**\n",
        "* Adjectives: happier, happiest → happy\n",
        "* Adverbs: worse, worst → badly\n",
        "* Nouns: dogs, children → dog, child\n",
        "* Verbs: writes, writing, wrote, written → write"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-mUzGGOGQZ5",
        "outputId": "ab6d9e6c-ed7d-4803-aa43-b0a2803bb655"
      },
      "source": [
        "word_list"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['organize', 'organizes', 'organizing']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "39f-TszPGUcH",
        "outputId": "b914988e-79cc-4df0-be12-fe4160dc927b"
      },
      "source": [
        "(\" \".join(word_list))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'organize organizes organizing'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kx0PmG44-HWP",
        "outputId": "a7e6c439-a3b7-4663-9682-197cb1ad9657"
      },
      "source": [
        "# Lemmatization using spacy\n",
        "for token in nlp(\" \".join(word_list)):\n",
        "    print(token.text, token.lemma_)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "organize organize\n",
            "organizes organize\n",
            "organizing organize\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ3gqmTx-HWQ"
      },
      "source": [
        "### Part-of-speech (POS) Tagging\n",
        "\n",
        "After tokenization, spaCy can parse and tag a given Doc. This is where the statistical model comes in, which enables spaCy to make a prediction of which tag or label most likely applies in this context. A model consists of binary data and is produced by showing a system enough examples for it to make predictions that generalize across the language – for example, a word following \"the\" in English is most likely a noun.\n",
        "\n",
        "Annotation | Description\n",
        ":----- |:------|\n",
        "Text |The original word text|\n",
        "Lemma |The base form of the word.|\n",
        "POS |The simple part-of-speech tag.|\n",
        "Tag |The detailed part-of-speech tag.|\n",
        "Dep |Syntactic dependency, i.e. the relation between tokens.|\n",
        "Shape |The word shape – capitalisation, punctuation, digits.|\n",
        "Is Alpha |Is the token an alpha character?|\n",
        "Is Stop |Is the token part of a stop list, i.e. the most common words of the language?|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Doiwt2u-HWS",
        "outputId": "44faac9a-7534-490c-9a3e-217bce396aca"
      },
      "source": [
        "# review document\n",
        "doc"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "This is one of the greatest films ever made. Brilliant acting by George C. Scott and Diane Riggs. This movie is both disturbing and extremely deep. Don't be fooled into believing this is just a comedy. It is a brilliant satire about the medical profession. It is not a pretty picture. Healthy patients are killed by incompetent surgeons, who spend all their time making money outside the hospital. And yet, you really believe that this is a hospital. The producers were very careful to include real medical terminology and real medical cases. This movie really reveals how difficult in is to run a hospital, and how badly things already were in 1971. I loved this movie."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w84kXrHK-HWT",
        "outputId": "5a270ff3-9ff9-43bb-a003-73e098c21cad"
      },
      "source": [
        "# check if POS tags were added to the doc in the NLP pipeline\n",
        "doc.is_tagged"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olfgCU65-HWU",
        "outputId": "1761ccd0-c86b-4ab4-8f8c-4ce410fe1068"
      },
      "source": [
        "# print column headers\n",
        "print('{:15} | {:15} | {:8} | {:8} | {:11} | {:8} | {:8} | {:8} | '.format(\n",
        "    'TEXT','LEMMA_','POS_','TAG_','DEP_','SHAPE_','IS_ALPHA','IS_STOP'))\n",
        "\n",
        "# print various SpaCy POS attributes\n",
        "for token in doc:\n",
        "    print('{:15} | {:15} | {:8} | {:8} | {:11} | {:8} | {:8} | {:8} |'.format(\n",
        "          token.text, token.lemma_, token.pos_, token.tag_, token.dep_\n",
        "        , token.shape_, token.is_alpha, token.is_stop))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TEXT            | LEMMA_          | POS_     | TAG_     | DEP_        | SHAPE_   | IS_ALPHA | IS_STOP  | \n",
            "This            | this            | DET      | DT       | nsubj       | Xxxx     |        1 |        1 |\n",
            "is              | be              | AUX      | VBZ      | ROOT        | xx       |        1 |        1 |\n",
            "one             | one             | NUM      | CD       | attr        | xxx      |        1 |        1 |\n",
            "of              | of              | ADP      | IN       | prep        | xx       |        1 |        1 |\n",
            "the             | the             | DET      | DT       | det         | xxx      |        1 |        1 |\n",
            "greatest        | great           | ADJ      | JJS      | amod        | xxxx     |        1 |        0 |\n",
            "films           | film            | NOUN     | NNS      | pobj        | xxxx     |        1 |        0 |\n",
            "ever            | ever            | ADV      | RB       | advmod      | xxxx     |        1 |        1 |\n",
            "made            | make            | VERB     | VBN      | acl         | xxxx     |        1 |        1 |\n",
            ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
            "Brilliant       | brilliant       | ADJ      | JJ       | amod        | Xxxxx    |        1 |        0 |\n",
            "acting          | act             | VERB     | VBG      | ROOT        | xxxx     |        1 |        0 |\n",
            "by              | by              | ADP      | IN       | prep        | xx       |        1 |        1 |\n",
            "George          | George          | PROPN    | NNP      | compound    | Xxxxx    |        1 |        0 |\n",
            "C.              | C.              | PROPN    | NNP      | compound    | X.       |        0 |        0 |\n",
            "Scott           | Scott           | PROPN    | NNP      | pobj        | Xxxxx    |        1 |        0 |\n",
            "and             | and             | CCONJ    | CC       | cc          | xxx      |        1 |        1 |\n",
            "Diane           | Diane           | PROPN    | NNP      | compound    | Xxxxx    |        1 |        0 |\n",
            "Riggs           | Riggs           | PROPN    | NNP      | conj        | Xxxxx    |        1 |        0 |\n",
            ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
            "This            | this            | DET      | DT       | det         | Xxxx     |        1 |        1 |\n",
            "movie           | movie           | NOUN     | NN       | nsubj       | xxxx     |        1 |        0 |\n",
            "is              | be              | AUX      | VBZ      | ROOT        | xx       |        1 |        1 |\n",
            "both            | both            | DET      | DT       | advmod      | xxxx     |        1 |        1 |\n",
            "disturbing      | disturbing      | ADJ      | JJ       | acomp       | xxxx     |        1 |        0 |\n",
            "and             | and             | CCONJ    | CC       | cc          | xxx      |        1 |        1 |\n",
            "extremely       | extremely       | ADV      | RB       | advmod      | xxxx     |        1 |        0 |\n",
            "deep            | deep            | ADJ      | JJ       | conj        | xxxx     |        1 |        0 |\n",
            ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
            "Do              | do              | AUX      | VB       | aux         | Xx       |        1 |        1 |\n",
            "n't             | not             | PART     | RB       | neg         | x'x      |        0 |        1 |\n",
            "be              | be              | AUX      | VB       | auxpass     | xx       |        1 |        1 |\n",
            "fooled          | fool            | VERB     | VBN      | ROOT        | xxxx     |        1 |        0 |\n",
            "into            | into            | ADP      | IN       | prep        | xxxx     |        1 |        1 |\n",
            "believing       | believe         | VERB     | VBG      | pcomp       | xxxx     |        1 |        0 |\n",
            "this            | this            | DET      | DT       | nsubj       | xxxx     |        1 |        1 |\n",
            "is              | be              | AUX      | VBZ      | ccomp       | xx       |        1 |        1 |\n",
            "just            | just            | ADV      | RB       | advmod      | xxxx     |        1 |        1 |\n",
            "a               | a               | DET      | DT       | det         | x        |        1 |        1 |\n",
            "comedy          | comedy          | NOUN     | NN       | attr        | xxxx     |        1 |        0 |\n",
            ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
            "It              | -PRON-          | PRON     | PRP      | nsubj       | Xx       |        1 |        1 |\n",
            "is              | be              | AUX      | VBZ      | ROOT        | xx       |        1 |        1 |\n",
            "a               | a               | DET      | DT       | det         | x        |        1 |        1 |\n",
            "brilliant       | brilliant       | ADJ      | JJ       | amod        | xxxx     |        1 |        0 |\n",
            "satire          | satire          | NOUN     | NN       | attr        | xxxx     |        1 |        0 |\n",
            "about           | about           | ADP      | IN       | prep        | xxxx     |        1 |        1 |\n",
            "the             | the             | DET      | DT       | det         | xxx      |        1 |        1 |\n",
            "medical         | medical         | ADJ      | JJ       | amod        | xxxx     |        1 |        0 |\n",
            "profession      | profession      | NOUN     | NN       | pobj        | xxxx     |        1 |        0 |\n",
            ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
            "It              | -PRON-          | PRON     | PRP      | nsubj       | Xx       |        1 |        1 |\n",
            "is              | be              | AUX      | VBZ      | ROOT        | xx       |        1 |        1 |\n",
            "not             | not             | PART     | RB       | neg         | xxx      |        1 |        1 |\n",
            "a               | a               | DET      | DT       | det         | x        |        1 |        1 |\n",
            "pretty          | pretty          | ADJ      | JJ       | amod        | xxxx     |        1 |        0 |\n",
            "picture         | picture         | NOUN     | NN       | attr        | xxxx     |        1 |        0 |\n",
            ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
            "Healthy         | healthy         | ADJ      | JJ       | amod        | Xxxxx    |        1 |        0 |\n",
            "patients        | patient         | NOUN     | NNS      | nsubjpass   | xxxx     |        1 |        0 |\n",
            "are             | be              | AUX      | VBP      | auxpass     | xxx      |        1 |        1 |\n",
            "killed          | kill            | VERB     | VBN      | ROOT        | xxxx     |        1 |        0 |\n",
            "by              | by              | ADP      | IN       | agent       | xx       |        1 |        1 |\n",
            "incompetent     | incompetent     | ADJ      | JJ       | amod        | xxxx     |        1 |        0 |\n",
            "surgeons        | surgeon         | NOUN     | NNS      | pobj        | xxxx     |        1 |        0 |\n",
            ",               | ,               | PUNCT    | ,        | punct       | ,        |        0 |        0 |\n",
            "who             | who             | PRON     | WP       | nsubj       | xxx      |        1 |        1 |\n",
            "spend           | spend           | VERB     | VBP      | relcl       | xxxx     |        1 |        0 |\n",
            "all             | all             | DET      | DT       | predet      | xxx      |        1 |        1 |\n",
            "their           | -PRON-          | DET      | PRP$     | poss        | xxxx     |        1 |        1 |\n",
            "time            | time            | NOUN     | NN       | dobj        | xxxx     |        1 |        0 |\n",
            "making          | make            | VERB     | VBG      | xcomp       | xxxx     |        1 |        0 |\n",
            "money           | money           | NOUN     | NN       | dobj        | xxxx     |        1 |        0 |\n",
            "outside         | outside         | ADP      | IN       | prep        | xxxx     |        1 |        0 |\n",
            "the             | the             | DET      | DT       | det         | xxx      |        1 |        1 |\n",
            "hospital        | hospital        | NOUN     | NN       | pobj        | xxxx     |        1 |        0 |\n",
            ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
            "And             | and             | CCONJ    | CC       | cc          | Xxx      |        1 |        1 |\n",
            "yet             | yet             | ADV      | RB       | advmod      | xxx      |        1 |        1 |\n",
            ",               | ,               | PUNCT    | ,        | punct       | ,        |        0 |        0 |\n",
            "you             | -PRON-          | PRON     | PRP      | nsubj       | xxx      |        1 |        1 |\n",
            "really          | really          | ADV      | RB       | advmod      | xxxx     |        1 |        1 |\n",
            "believe         | believe         | VERB     | VBP      | ROOT        | xxxx     |        1 |        0 |\n",
            "that            | that            | SCONJ    | IN       | mark        | xxxx     |        1 |        1 |\n",
            "this            | this            | DET      | DT       | nsubj       | xxxx     |        1 |        1 |\n",
            "is              | be              | AUX      | VBZ      | ccomp       | xx       |        1 |        1 |\n",
            "a               | a               | DET      | DT       | det         | x        |        1 |        1 |\n",
            "hospital        | hospital        | NOUN     | NN       | attr        | xxxx     |        1 |        0 |\n",
            ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
            "The             | the             | DET      | DT       | det         | Xxx      |        1 |        1 |\n",
            "producers       | producer        | NOUN     | NNS      | nsubj       | xxxx     |        1 |        0 |\n",
            "were            | be              | AUX      | VBD      | ROOT        | xxxx     |        1 |        1 |\n",
            "very            | very            | ADV      | RB       | advmod      | xxxx     |        1 |        1 |\n",
            "careful         | careful         | ADJ      | JJ       | acomp       | xxxx     |        1 |        0 |\n",
            "to              | to              | PART     | TO       | aux         | xx       |        1 |        1 |\n",
            "include         | include         | VERB     | VB       | xcomp       | xxxx     |        1 |        0 |\n",
            "real            | real            | ADJ      | JJ       | amod        | xxxx     |        1 |        0 |\n",
            "medical         | medical         | ADJ      | JJ       | amod        | xxxx     |        1 |        0 |\n",
            "terminology     | terminology     | NOUN     | NN       | dobj        | xxxx     |        1 |        0 |\n",
            "and             | and             | CCONJ    | CC       | cc          | xxx      |        1 |        1 |\n",
            "real            | real            | ADJ      | JJ       | amod        | xxxx     |        1 |        0 |\n",
            "medical         | medical         | ADJ      | JJ       | amod        | xxxx     |        1 |        0 |\n",
            "cases           | case            | NOUN     | NNS      | conj        | xxxx     |        1 |        0 |\n",
            ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
            "This            | this            | DET      | DT       | det         | Xxxx     |        1 |        1 |\n",
            "movie           | movie           | NOUN     | NN       | nsubj       | xxxx     |        1 |        0 |\n",
            "really          | really          | ADV      | RB       | advmod      | xxxx     |        1 |        1 |\n",
            "reveals         | reveal          | VERB     | VBZ      | ROOT        | xxxx     |        1 |        0 |\n",
            "how             | how             | ADV      | WRB      | advmod      | xxx      |        1 |        1 |\n",
            "difficult       | difficult       | ADJ      | JJ       | acomp       | xxxx     |        1 |        0 |\n",
            "in              | in              | ADP      | IN       | expl        | xx       |        1 |        1 |\n",
            "is              | be              | AUX      | VBZ      | ccomp       | xx       |        1 |        1 |\n",
            "to              | to              | PART     | TO       | aux         | xx       |        1 |        1 |\n",
            "run             | run             | VERB     | VB       | xcomp       | xxx      |        1 |        0 |\n",
            "a               | a               | DET      | DT       | det         | x        |        1 |        1 |\n",
            "hospital        | hospital        | NOUN     | NN       | dobj        | xxxx     |        1 |        0 |\n",
            ",               | ,               | PUNCT    | ,        | punct       | ,        |        0 |        0 |\n",
            "and             | and             | CCONJ    | CC       | cc          | xxx      |        1 |        1 |\n",
            "how             | how             | ADV      | WRB      | advmod      | xxx      |        1 |        1 |\n",
            "badly           | badly           | ADV      | RB       | advmod      | xxxx     |        1 |        0 |\n",
            "things          | thing           | NOUN     | NNS      | nsubj       | xxxx     |        1 |        0 |\n",
            "already         | already         | ADV      | RB       | advmod      | xxxx     |        1 |        1 |\n",
            "were            | be              | AUX      | VBD      | conj        | xxxx     |        1 |        1 |\n",
            "in              | in              | ADP      | IN       | prep        | xx       |        1 |        1 |\n",
            "1971            | 1971            | NUM      | CD       | pobj        | dddd     |        0 |        0 |\n",
            ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
            "I               | -PRON-          | PRON     | PRP      | nsubj       | X        |        1 |        1 |\n",
            "loved           | love            | VERB     | VBD      | ROOT        | xxxx     |        1 |        0 |\n",
            "this            | this            | DET      | DT       | det         | xxxx     |        1 |        1 |\n",
            "movie           | movie           | NOUN     | NN       | dobj        | xxxx     |        1 |        0 |\n",
            ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xR5riwQx-HWV",
        "outputId": "9a5396af-7faa-4181-fbc0-61bde5879a59"
      },
      "source": [
        "spacy.explain('JJ')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'adjective'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNFO51dsIIsw",
        "outputId": "0efdf358-c35a-4252-f69a-1f9bb8d7cbd2"
      },
      "source": [
        "doc[0]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "This"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw8hP6xJ-HWW",
        "outputId": "99096163-693b-405e-c0e4-18f8a69e6f9e"
      },
      "source": [
        "previous_token = doc[0]  # set first token\n",
        "\n",
        "for token in doc[1:]:    \n",
        "    # identify adjective noun pairs\n",
        "    if previous_token.pos_ == 'ADJ' and token.pos_ == 'NOUN':\n",
        "        print(f'{previous_token.text}_{token.text}')\n",
        "    \n",
        "    previous_token = token"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "greatest_films\n",
            "brilliant_satire\n",
            "medical_profession\n",
            "pretty_picture\n",
            "Healthy_patients\n",
            "incompetent_surgeons\n",
            "medical_terminology\n",
            "medical_cases\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZUuxT2F-HWX"
      },
      "source": [
        "##### word sense disambiguation via part of speech tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rj1qmCNf-HWX",
        "outputId": "1c5bf58c-c885-4940-f570-3d111d36b793"
      },
      "source": [
        "for token in doc[0:20]:\n",
        "    print(f'{token.text}_{token.pos_}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This_DET\n",
            "is_AUX\n",
            "one_NUM\n",
            "of_ADP\n",
            "the_DET\n",
            "greatest_ADJ\n",
            "films_NOUN\n",
            "ever_ADV\n",
            "made_VERB\n",
            "._PUNCT\n",
            "Brilliant_ADJ\n",
            "acting_VERB\n",
            "by_ADP\n",
            "George_PROPN\n",
            "C._PROPN\n",
            "Scott_PROPN\n",
            "and_CCONJ\n",
            "Diane_PROPN\n",
            "Riggs_PROPN\n",
            "._PUNCT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHnWewsG-HWY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNXEWnsq-HWZ",
        "outputId": "43454c99-c23a-4c1a-b577-8b44d8512912"
      },
      "source": [
        "spacy.explain('CARDINAL')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Numerals that do not fall under another type'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6jCgTCQ-HWZ"
      },
      "source": [
        "### Named Entity Recognition (NER)\n",
        "\n",
        "A named entity is a \"real-world object\" that's assigned a name – for example, a person, a country, a product, or a book title. spaCy can recognise various types of named entities in a document, by asking the model for a prediction. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiXwPT8P-HWa"
      },
      "source": [
        "ner_text = \"When I told John that I wanted to move to Alaska, he warned me that I'd have trouble finding a Starbucks there.\"\n",
        "ner_doc = nlp(ner_text)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjCxWZOj-HWa",
        "outputId": "37e733c4-4874-43c4-b96b-41f7191a0e53"
      },
      "source": [
        "print('{:10} | {:15}'.format('LABEL','ENTITY'))\n",
        "\n",
        "for ent in ner_doc.ents[0:20]:\n",
        "    print('{:10} | {:50}'.format(ent.label_, ent.text))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LABEL      | ENTITY         \n",
            "PERSON     | John                                              \n",
            "GPE        | Alaska                                            \n",
            "ORG        | Starbucks                                         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAECBeju-HWe",
        "outputId": "07f19010-1740-4ef7-e2de-87bcd4531213"
      },
      "source": [
        "# ent methods and attributes\n",
        "print(dir(ent))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['_', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_fix_dep_copy', '_recalculate_indices', '_vector', '_vector_norm', 'as_doc', 'char_span', 'conjuncts', 'doc', 'end', 'end_char', 'ent_id', 'ent_id_', 'ents', 'get_extension', 'get_lca_matrix', 'has_extension', 'has_vector', 'kb_id', 'kb_id_', 'label', 'label_', 'lefts', 'lemma_', 'lower_', 'merge', 'n_lefts', 'n_rights', 'noun_chunks', 'orth_', 'remove_extension', 'rights', 'root', 'sent', 'sentiment', 'set_extension', 'similarity', 'start', 'start_char', 'string', 'subtree', 'tensor', 'text', 'text_with_ws', 'to_array', 'upper_', 'vector', 'vector_norm', 'vocab']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "voYnqMz7-HWf",
        "outputId": "f0778c9a-c3d2-470a-c9a8-5c4193e1ae83"
      },
      "source": [
        "displacy.render(docs=ner_doc, style='ent', jupyter=True)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">When I told \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    John\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " that I wanted to move to \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Alaska\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", he warned me that I'd have trouble finding a \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Starbucks\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " there.</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NRuQQN6C-HWg",
        "outputId": "db053c9f-4a92-4317-a2d3-b2a97b522905"
      },
      "source": [
        "spacy.explain('GPE')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Countries, cities, states'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "nJ4frF36-HWh",
        "outputId": "e870e9de-2f73-43f1-f1d8-3cd5f6318ff4"
      },
      "source": [
        "doc1 = nlp(\"Larry Page founded Google\") #\"Apple is looking at buying U.K. startup for $1 billion\"\n",
        "displacy.render(doc1, style=\"ent\",jupyter=True)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Larry Page\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " founded \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Google\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ay5TkjF4JFdl",
        "outputId": "f315891d-741e-44f3-c63a-3f3e225e3e37"
      },
      "source": [
        "doc1 = nlp(\"Narendra Modi is Prime Minister of India\") #\"Apple is looking at buying U.K. startup for $1 billion\"\n",
        "displacy.render(doc1, style=\"ent\",jupyter=True)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Narendra Modi\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " is Prime Minister of \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    India\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              "</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "ARkYVm3_-HWi",
        "outputId": "b4ef9e8c-becc-4d80-b3f1-74c0031cb034"
      },
      "source": [
        "# dependency visualization\n",
        "\n",
        "# show visualization in Jupyter Notebook\n",
        "displacy.render(docs=doc, style='ent', jupyter=True)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">This is one of the greatest films ever made. Brilliant acting by \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    George C. Scott\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " and \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Diane Riggs\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ". This movie is both disturbing and extremely deep. Don't be fooled into believing this is just a comedy. It is a brilliant satire about the medical profession. It is not a pretty picture. Healthy patients are killed by incompetent surgeons, who spend all their time making money outside the hospital. And yet, you really believe that this is a hospital. The producers were very careful to include real medical terminology and real medical cases. This movie really reveals how difficult in is to run a hospital, and how badly things already were in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    1971\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ". I loved this movie.</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56q6p1-A-HWl"
      },
      "source": [
        "### Fast Sentence Boundary Detection (SBD)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z60fSAE9-HWm",
        "outputId": "378386cd-dba0-49c9-ea91-11ade20c6c77"
      },
      "source": [
        "\"This is a sentence. This is another sentence. let's go to N.Y.!\".split('.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This is a sentence', ' This is another sentence', \" let's go to N\", 'Y', '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUo8QkEJ-HWn",
        "outputId": "aac8acfc-5817-474f-b0f3-7690f70d6446"
      },
      "source": [
        "doc = nlp(\"This is a sentence. This is another sentence. let's go to N.Y.!\") \n",
        "\n",
        "for sent in doc.sents:\n",
        "    print(sent.text)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a sentence.\n",
            "This is another sentence.\n",
            "let's go to N.Y.!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcOOUKQr-HWp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-qsW9vg-HWp"
      },
      "source": [
        "tokens = ['i', 'want', 'to', 'go', 'to', 'school'] # \"i_want\", want_to"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RJ_Vfv_-HWq"
      },
      "source": [
        "def ngrams(tokens, n):\n",
        "    length = len(tokens)\n",
        "    grams = []\n",
        "    for i in range(length - n + 1):\n",
        "        grams.append(\"_\".join(tokens[i:i+n]))\n",
        "    return grams"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LopGJXMx-HWr",
        "outputId": "3b24bf82-e841-4b05-a5e9-88d9a3d8427f"
      },
      "source": [
        "spacy.load('en')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7f8cac4bab10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra-sEGEF-HWs",
        "outputId": "1b3b6272-5401-47e0-99e3-b8f4b26c2ddb"
      },
      "source": [
        "print(ngrams(tokens, 2))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i_want', 'want_to', 'to_go', 'go_to', 'to_school']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QfBCYGoJpmG",
        "outputId": "415e1e6f-ffce-4424-ccc8-dce3dc3f5d23"
      },
      "source": [
        "print(ngrams(tokens, 3))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i_want_to', 'want_to_go', 'to_go_to', 'go_to_school']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_cueGQcJ3-O"
      },
      "source": [
        "Link: https://course.spacy.io/en/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iro4MZHs-HWs"
      },
      "source": [
        "### Python Fundamentals\n",
        "A brief overview of some advanced Python which will be used in future lessons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCYOPnXY-HWt"
      },
      "source": [
        "from collections import defaultdict,Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FUOaQkX-HWt"
      },
      "source": [
        "review = \"\"\"This is one of the greatest films ever made. Brilliant acting by George C. Scott and Diane Riggs.\n",
        "This movie is both disturbing and extremely deep. Don't be fooled into believing this is just a comedy. \n",
        "It is a brilliant satire about the medical profession. It is not a pretty picture. Healthy patients are killed by incompetent surgeons, who spend all their time making money outside the hospital. And yet, you really believe that this is a hospital. The producers were very careful to include real medical terminology and real medical cases.\n",
        "This movie really reveals how difficult in is to run a hospital, and how badly things already were in 1971. I loved this movie.\"\"\".strip()\n",
        "\n",
        "example_doc = nlp(review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gnjt648-HWu",
        "outputId": "605fbc19-e32e-4d1d-9eab-ba939f49fb98"
      },
      "source": [
        "# WRONG APPROACH - KeyError!\n",
        "\n",
        "# try to create a word count dict with new keys\n",
        "vocab = {}\n",
        "for word in example_doc:\n",
        "    try:\n",
        "        vocab[word.text] += 1\n",
        "    except:\n",
        "        vocab[word.text] = 1\n",
        "    \n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'This': 3, 'is': 7, 'one': 1, 'of': 1, 'the': 3, 'greatest': 1, 'films': 1, 'ever': 1, 'made': 1, '.': 11, 'Brilliant': 1, 'acting': 1, 'by': 2, 'George': 1, 'C.': 1, 'Scott': 1, 'and': 4, 'Diane': 1, 'Riggs': 1, '\\n': 3, 'movie': 3, 'both': 1, 'disturbing': 1, 'extremely': 1, 'deep': 1, 'Do': 1, \"n't\": 1, 'be': 1, 'fooled': 1, 'into': 1, 'believing': 1, 'this': 3, 'just': 1, 'a': 5, 'comedy': 1, 'It': 2, 'brilliant': 1, 'satire': 1, 'about': 1, 'medical': 3, 'profession': 1, 'not': 1, 'pretty': 1, 'picture': 1, 'Healthy': 1, 'patients': 1, 'are': 1, 'killed': 1, 'incompetent': 1, 'surgeons': 1, ',': 3, 'who': 1, 'spend': 1, 'all': 1, 'their': 1, 'time': 1, 'making': 1, 'money': 1, 'outside': 1, 'hospital': 3, 'And': 1, 'yet': 1, 'you': 1, 'really': 2, 'believe': 1, 'that': 1, 'The': 1, 'producers': 1, 'were': 2, 'very': 1, 'careful': 1, 'to': 2, 'include': 1, 'real': 2, 'terminology': 1, 'cases': 1, 'reveals': 1, 'how': 2, 'difficult': 1, 'in': 2, 'run': 1, 'badly': 1, 'things': 1, 'already': 1, '1971': 1, 'I': 1, 'loved': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqFBV9_U-HWv"
      },
      "source": [
        "??defaultdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOX93wXj-HWw",
        "outputId": "ff4aca79-cd03-4c84-b626-a1e7aebf6f6c"
      },
      "source": [
        "d = defaultdict(int)  # define the type of data the dict stores\n",
        "\n",
        "for word in example_doc:\n",
        "    d[word.text] += 1  # can add to unassigned keys\n",
        "\n",
        "print(d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<class 'int'>, {'This': 3, 'is': 7, 'one': 1, 'of': 1, 'the': 3, 'greatest': 1, 'films': 1, 'ever': 1, 'made': 1, '.': 11, 'Brilliant': 1, 'acting': 1, 'by': 2, 'George': 1, 'C.': 1, 'Scott': 1, 'and': 4, 'Diane': 1, 'Riggs': 1, '\\n': 3, 'movie': 3, 'both': 1, 'disturbing': 1, 'extremely': 1, 'deep': 1, 'Do': 1, \"n't\": 1, 'be': 1, 'fooled': 1, 'into': 1, 'believing': 1, 'this': 3, 'just': 1, 'a': 5, 'comedy': 1, 'It': 2, 'brilliant': 1, 'satire': 1, 'about': 1, 'medical': 3, 'profession': 1, 'not': 1, 'pretty': 1, 'picture': 1, 'Healthy': 1, 'patients': 1, 'are': 1, 'killed': 1, 'incompetent': 1, 'surgeons': 1, ',': 3, 'who': 1, 'spend': 1, 'all': 1, 'their': 1, 'time': 1, 'making': 1, 'money': 1, 'outside': 1, 'hospital': 3, 'And': 1, 'yet': 1, 'you': 1, 'really': 2, 'believe': 1, 'that': 1, 'The': 1, 'producers': 1, 'were': 2, 'very': 1, 'careful': 1, 'to': 2, 'include': 1, 'real': 2, 'terminology': 1, 'cases': 1, 'reveals': 1, 'how': 2, 'difficult': 1, 'in': 2, 'run': 1, 'badly': 1, 'things': 1, 'already': 1, '1971': 1, 'I': 1, 'loved': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88J86Iy5-HWx",
        "outputId": "167841ae-2215-4ab8-e684-0cd5def2e915"
      },
      "source": [
        "somedict = {'a':2}\n",
        "print(somedict[3]) # KeyError\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "3",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-ce7cf1d72652>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msomedict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msomedict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# KeyError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 3"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mua5YHfW-HWy",
        "outputId": "8ebddef8-3d7f-40ae-93d5-739b2c56bc73"
      },
      "source": [
        "someddict = defaultdict(int)\n",
        "print(someddict[3]) # print int(), thus 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSKBL0r2-HWz",
        "outputId": "42111fa5-419c-4acc-a26e-1236abd414e4"
      },
      "source": [
        "# count the number of times each CARDINAL appears\n",
        "print(Counter(d))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'.': 11, 'is': 7, 'a': 5, 'and': 4, 'This': 3, 'the': 3, '\\n': 3, 'movie': 3, 'this': 3, 'medical': 3, ',': 3, 'hospital': 3, 'by': 2, 'It': 2, 'really': 2, 'were': 2, 'to': 2, 'real': 2, 'how': 2, 'in': 2, 'one': 1, 'of': 1, 'greatest': 1, 'films': 1, 'ever': 1, 'made': 1, 'Brilliant': 1, 'acting': 1, 'George': 1, 'C.': 1, 'Scott': 1, 'Diane': 1, 'Riggs': 1, 'both': 1, 'disturbing': 1, 'extremely': 1, 'deep': 1, 'Do': 1, \"n't\": 1, 'be': 1, 'fooled': 1, 'into': 1, 'believing': 1, 'just': 1, 'comedy': 1, 'brilliant': 1, 'satire': 1, 'about': 1, 'profession': 1, 'not': 1, 'pretty': 1, 'picture': 1, 'Healthy': 1, 'patients': 1, 'are': 1, 'killed': 1, 'incompetent': 1, 'surgeons': 1, 'who': 1, 'spend': 1, 'all': 1, 'their': 1, 'time': 1, 'making': 1, 'money': 1, 'outside': 1, 'And': 1, 'yet': 1, 'you': 1, 'believe': 1, 'that': 1, 'The': 1, 'producers': 1, 'very': 1, 'careful': 1, 'include': 1, 'terminology': 1, 'cases': 1, 'reveals': 1, 'difficult': 1, 'run': 1, 'badly': 1, 'things': 1, 'already': 1, '1971': 1, 'I': 1, 'loved': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVH2eIkr-HW0",
        "outputId": "e41a6265-ce8a-40a5-c3c7-8238f62e70ad"
      },
      "source": [
        "most_common=Counter(d).most_common(10)\n",
        "most_common"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('.', 11),\n",
              " ('is', 7),\n",
              " ('a', 5),\n",
              " ('and', 4),\n",
              " ('This', 3),\n",
              " ('the', 3),\n",
              " ('\\n', 3),\n",
              " ('movie', 3),\n",
              " ('this', 3),\n",
              " ('medical', 3)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlQItOrl-HW1",
        "outputId": "2812e912-47ee-4a03-f80f-8c418c33d754"
      },
      "source": [
        "most_common=Counter(review.split()).most_common(4)\n",
        "most_common"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('is', 7), ('a', 5), ('and', 4), ('This', 3)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnd15Vp1-HW1"
      },
      "source": [
        "#### **LIST ** ****\n",
        "unpacking, slicing, "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85-bpvI2-HW1",
        "outputId": "29c41dc0-3d1a-40ab-c85e-b8cc6ef9501f"
      },
      "source": [
        "elems = [1, 2, 3, 4]\n",
        "a, b, c, d = elems\n",
        "print(a, b, c, d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 2 3 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4O8dLmf-HW2",
        "outputId": "4990d080-1d0a-4021-bfd4-82c59ec986cf"
      },
      "source": [
        "a, *new_elems, d = elems\n",
        "print(a)\n",
        "print(new_elems)\n",
        "print(d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "[2, 3]\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n43kSNoF-HW3",
        "outputId": "45cf844d-7cfd-47af-80f3-15578eb0d054"
      },
      "source": [
        "elems = list(range(10))\n",
        "print(elems)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPBQam4i-HW4",
        "outputId": "6be2d946-2ce6-4ba9-956e-3c81156479cf"
      },
      "source": [
        "print(elems[::-1]) # 9 8 76"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP1aW1R--HW5",
        "outputId": "1ae84acb-416c-49e0-9b27-9fa03d832c6d"
      },
      "source": [
        "elems[::2],elems[-2::-2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 2, 4, 6, 8], [8, 6, 4, 2, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikz79KdZ-HW6"
      },
      "source": [
        "#### List Comprehension"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmV_WlUD-HW7",
        "outputId": "df2ab9f5-16bd-4d83-b416-deab14341fcc"
      },
      "source": [
        "nums = [1,2,3,4,5]\n",
        "nums_squared = [num * num for num in nums if num %2==0]\n",
        "print(nums_squared)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4, 16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJsA4ktB-HW8",
        "outputId": "19906349-fa22-4640-e167-7cc7071fe11e"
      },
      "source": [
        "nums_squared=[]\n",
        "for num in nums:\n",
        "    nums_squared.append(num)\n",
        "\n",
        "nums_squared"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoAhFjXM-HW8"
      },
      "source": [
        " #### Lambda, map, filter, reduce"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BzILKPv-HW9"
      },
      "source": [
        "def square_fn(x):\n",
        "    return x * x\n",
        "\n",
        "square_ld = lambda x: x * x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT0sm-xE-HW9",
        "outputId": "5c458147-2438-417e-e91b-fed5dfb764ae"
      },
      "source": [
        "# square_fn(5)\n",
        "square_ld(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVqBUf4o-HW-",
        "outputId": "5171cacc-84c4-40d6-f473-4c647993a186"
      },
      "source": [
        "nums"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVb8l63N-HW-",
        "outputId": "83c561d2-992f-4903-d5fc-cf893ed743f2"
      },
      "source": [
        "nums_squared_1 = map(square_fn, nums)\n",
        "nums_squared_2 = map(lambda x: x * x, nums)\n",
        "print(list(nums_squared_1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 4, 9, 16, 25]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6pMhRVA-HXB",
        "outputId": "7e0adc7f-25a0-41c6-fac3-d7ccf5aebd7d"
      },
      "source": [
        "filtered_Values = filter(lambda x: x > 10, nums)\n",
        "print(list(filtered_Values))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFvsI2Bh-HXC"
      },
      "source": [
        "#### IMDB REVIEW DATA EXPLORATION "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "4A-KhBKs-HXD",
        "outputId": "e39ca717-4bcb-45bb-b5d2-1ded0ea07fbf"
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/kaggle/input/usinlppracticum/sample_submission.csv\n",
            "/kaggle/input/usinlppracticum/imdb_test.csv\n",
            "/kaggle/input/usinlppracticum/imdb_train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "DfmPTqxl-HXE",
        "outputId": "4305c4cf-9302-46e4-ddea-9247fd8db254"
      },
      "source": [
        "data= pd.read_csv('/kaggle/input/usinlppracticum/imdb_train.csv')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>We had STARZ free weekend and I switched on th...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I'll admit that this isn't a great film. It pr...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I finally found a version of Persuasion that I...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The BBC surpassed themselves with the boundari...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Much praise has been lavished upon Farscape, b...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  We had STARZ free weekend and I switched on th...  negative\n",
              "1  I'll admit that this isn't a great film. It pr...  negative\n",
              "2  I finally found a version of Persuasion that I...  positive\n",
              "3  The BBC surpassed themselves with the boundari...  positive\n",
              "4  Much praise has been lavished upon Farscape, b...  negative"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMYSG8BK-HXE",
        "outputId": "a3941254-5d30-4bf1-eee9-a3586d0b8a67"
      },
      "source": [
        "data.iloc[1,0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I\\'ll admit that this isn\\'t a great film. It practically screams \"low-budget\" yet oddly I still found myself liking the film because although it lacked quality it abounded with energy. It was like the Little Engine That Could and a movie merged into one! <br /><br />The film takes place at a radio network and concerns some of their low-level employees--two page boys (one very pushy and brash and the other one a wuss) as well as a new receptionist. All three have visions of radio stardom but must for now content themselves with their lowly jobs.<br /><br />Into this story appears a murder that seems somewhat out of the blue. I didn\\'t know that this was a murder mystery film and was taken a bit by surprise. However, like most B-mysteries, the cops are lamebrains and it\\'s up to our pushy hero (Moran) to try to save the day. Throughout all this, I had a hard time deciding if Moran was obnoxious or endearing. I\\'m still not sure!! <br /><br />There is a moment in the film that is high on the \\'cringe factor\\' and that is when the two pages try out for the roles of radio comedians. They show up in black-face and do a 3rd or 4th rate imitation of Amos n\\' Andy. Apart from being very insensitive, it also wasn\\'t funny. Fortunately the producer of the show they were trying out for seemed to feel the same way.<br /><br />Overall. it\\'s easy to skip this film and I wouldn\\'t blame you if you do. However, the weird and frenetic pace of the film actually seemed to make up for the artistic deficiencies of the film and I am glad I saw it. A good film? No. But one that is still worth a peek for fans of old Bs.<br /><br />By the way, perhaps I just don\\'t have very good taste, but I thought BOTH female singing divas really had poor voices despite how everyone in the film is captivated by their warblings. Listen for yourself and let me know what you think. I just couldn\\'t believe either was allowed to sing on film--even if it was just for lowly Monogram Studios.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdzP0GzN-HXF"
      },
      "source": [
        "##### Pandas Apply\n",
        "\n",
        "apply is an efficient and fast approach to 'apply' a function to every element in a row. applymap does the same to every element in the entire dataframe (e.g. convert all ints to floats)\n",
        "\n",
        "Example: https://chrisalbon.com/python/data_wrangling/pandas_apply_operations_to_dataframes/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coDkBQR9-HXF",
        "outputId": "2320d04e-a549-4c1d-e722-0290b7c9f21f"
      },
      "source": [
        "# create a small dataframe with example data\n",
        "example_data = {'col1':range(0,3),'col2':range(3,6)}\n",
        "test_df = pd.DataFrame(example_data)\n",
        "test_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>col1</th>\n",
              "      <th>col2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   col1  col2\n",
              "0     0     3\n",
              "1     1     4\n",
              "2     2     5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J19Bq-8a-HXG",
        "outputId": "516559d2-c191-43fe-935a-ecdff3bfcbfb"
      },
      "source": [
        "# apply a built-in function to each element in a column\n",
        "test_df['col1'].apply(float)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.0\n",
              "1    1.0\n",
              "2    2.0\n",
              "Name: col1, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVKW7CA3-HXH",
        "outputId": "7c5500c5-c4a6-4bbb-e870-0cac5ccc535e"
      },
      "source": [
        "# apply a custom function to every element in a column\n",
        "def add_five(row):\n",
        "    return row + 5\n",
        "\n",
        "test_df['col1'].apply(add_five)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    5\n",
              "1    6\n",
              "2    7\n",
              "Name: col1, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjGTgLoQ-HXH",
        "outputId": "2101cf01-23d5-4b55-fd31-c51eaacf97d4"
      },
      "source": [
        "test_df['col1'].apply(lambda x: x+5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    5\n",
              "1    6\n",
              "2    7\n",
              "Name: col1, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQJ8YOcZ-HXI",
        "outputId": "066a5548-c96f-48a4-b764-acc6fc9d3d06"
      },
      "source": [
        "# apply an annonomous function to every element in a column\n",
        "test_df['col1'].apply(lambda x: x+5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    5\n",
              "1    6\n",
              "2    7\n",
              "Name: col1, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP2tx65g-HXI"
      },
      "source": [
        "##### Sorted\n",
        "\n",
        "sorted(iterable, key=None, reverse=False)\n",
        "\n",
        "- Return a new sorted list from the items in iterable.\n",
        "- Has two optional arguments which must be specified as keyword arguments.\n",
        "- key specifies a function of one argument that is used to extract a comparison key from each list element: key=str.lower. The default value is None (compare the elements directly).\n",
        "- reverse is a boolean value. If set to True, then the list elements are sorted as if each comparison were reversed.\n",
        "\n",
        "SOURCE: https://docs.python.org/3/library/functions.html#sorted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8Hrxaw3-HXJ"
      },
      "source": [
        "articles =[('article2', 3, 'za'),('article3', 2, 'yb'),('article1', 1, 'xc')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt90e8Js-HXJ",
        "outputId": "674f9ea4-35d3-4766-80b1-9663a993dc7a"
      },
      "source": [
        "sorted(articles)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('article1', 1, 'xc'), ('article2', 3, 'za'), ('article3', 2, 'yb')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXZiGiAO-HXK",
        "outputId": "88dd4857-197d-400b-bbca-a36fd1315e63"
      },
      "source": [
        "sorted(articles, key=lambda x: x[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('article1', 1, 'xc'), ('article3', 2, 'yb'), ('article2', 3, 'za')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zO6cbiB-HXK",
        "outputId": "64248b71-65a7-4f68-b93c-fa13f5cf4a3c"
      },
      "source": [
        "# sort based on the last term\n",
        "sorted(articles, key=lambda x: x[2][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('article2', 3, 'za'), ('article3', 2, 'yb'), ('article1', 1, 'xc')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVI8lAz_-HXL"
      },
      "source": [
        "imdb_sample=data.sample(1000).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6bpmIes-HXL"
      },
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9JE9FFy-HXM"
      },
      "source": [
        "imdb - apply (len, tokenize )\n",
        "value_counts,\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DnCwIVE-HXM"
      },
      "source": [
        "def clean_review(x):\n",
        "    doc = nlp(x) #test_df['col1'].apply(add_five)\n",
        "    narrative = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-'] \n",
        "    narrative = [i for i in narrative if i not in STOP_WORDS]#  removing # list comprehension\n",
        "    return \" \".join(narrative)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4KgH56T-HXN",
        "outputId": "4fde9833-67b0-4116-99be-8236e830d6ac"
      },
      "source": [
        "%%time\n",
        "imdb_sample['clean']=imdb_sample['review'].apply(lambda x:clean_review(x)) #progress_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 58.2 s, sys: 704 ms, total: 58.9 s\n",
            "Wall time: 56.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7opD2GA-HXO"
      },
      "source": [
        "**Next Lesson**\n",
        "* Text Vectorization\n",
        "* [Regex cheatsheet](https://www.cheatography.com/davechild/cheat-sheets/regular-expressions/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "romzrSQ9-HXO"
      },
      "source": [
        "### References:\n",
        "* https://spacy.io/api/language\n",
        "* https://www.fast.ai/2019/07/08/fastai-nlp/\n",
        "\n"
      ]
    }
  ]
}